# Simple pyspark quries
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

spark

data_path = 'Data'

file_path = data_path + "/location_temp.csv"
df1 = spark.read.format("csv").option("header", "true").load(file_path)

df1.show()

df1.count()

file_path_no_header = data_path + "/utilization.csv"
df2 = spark.read.format("csv").option("header","false").option("inferSchema","true").load(file_path_no_header)

df2.show()

df2 = df2.withColumnRenamed("_c0", "event_datetime") \
          .withColumnRenamed("_c1", "server_id") \
           .withColumnRenamed("_c2", "cpu_utilization") \
            .withColumnRenamed("_c3", "free_memory") \
             .withColumnRenamed("_c4", "session_count")

df2.show()

df1.filter(df1["location_id"]=='loc0').show()

df1.filter(df1["location_id"]=='loc0').count()

df1.filter(df1["location_id"]=='loc1').count()

df1.filter(df1["location_id"]=='loc1').show()

df1.groupBy("location_id").count().show()

df1.orderBy("location_id").show()

df1.groupBy("location_id").agg({'temp_celcius': 'mean'}).show()

df1.groupBy("location_id").agg({'temp_celcius': 'max'}).show()

df1.count()

df1_s1 = df1.sample(fraction=0.1, withReplacement=False)

df1_s1.count()

df1_s1.groupBy("location_id").agg({"temp_celcius": 'mean'}).show()

df1_s1.groupBy("location_id").agg({"temp_celcius": 'mean'}).orderBy("location_id").show(10)

df1.groupBy("location_id").agg({"temp_celcius": 'mean'}).orderBy("location_id").show(10)

df1.write.csv('df2.csv')

ls df2.csv

head df2.csv/part-00000-dc83ced0-9016-4a17-b2e6-e23f0262fb46-c000.csv

#Pyspark with json data

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

data_path  = 'Data'

json_df1_path = data_path + "/utilization.json"
df1 = spark.read.format("json").load(json_df1_path)

df1.show()

df1.count()

df1.columns

df1_sample = df1.sample(False, fraction=0.1)

df1_sample.show()

df1_sort = df1_sample.sort("event_datetime")

df1_sort.show()

# Handling NAValues and dropDuplicates

from pyspark.sql import Row
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
from pyspark.sql.types import StringType

df_dup = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80),\
                        Row(server_name='101 Server', cpu_utilization=80, session_count=90),\
                        Row(server_name='102 Server', cpu_utilization=85, session_count=80),\
                        Row(server_name='102 Server', cpu_utilization=85, session_count=80)]).toDF()

df_dup.show()

df_dup.drop_duplicates().show()

df_dup.drop_duplicates(['server_name']).show()

df = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80),\
                        Row(server_name='101 Server', cpu_utilization=80, session_count=90),\
                        Row(server_name='102 Server', cpu_utilization=85, session_count=40),\
                        Row(server_name='103 Server', cpu_utilization=70, session_count=80),\
                        Row(server_name='104 Server', cpu_utilization=60, session_count=80)]).toDF()

df.show()

df_na = df.withColumn('na_col', lit(None).cast(StringType()))

df_na.show()

df_na.fillna('A').show()

df2 = df_na.fillna('A').union(df_na)

df2.show()

df2.na.drop().show()

df2.createOrReplaceTempView("na_table")

spark.sql("SELECT * FROM na_table").show()

spark.sql("SELECT * FROM na_table WHERE na_col IS NULL").show()

spark.sql("SELECT * FROM na_table WHERE na_col IS NOT NULL").show()

# pyspark SqlDataframe

from pyspark.sql import SparkSession

spark = SparkSession\
        .builder\
        .appName("Saprk SQL Query Dataframes")\
        .getOrCreate()

data_path  = 'Data'

json_df1_path = data_path + "/utilization.json"
df1 = spark.read.format("json").load(json_df1_path)

df1.show(10)

df1.createOrReplaceTempView("utilization")

df1_sql = spark.sql("SELECT * FROM utilization LIMIT 10")

df1_sql.show()

df1_sql.count()

df1_sql = spark.sql("SELECT server_id, session_count FROM utilization LIMIT 10")
df1_sql.show()

# using alias
df1_sql = spark.sql("SELECT server_id AS sid, session_count AS sc FROM utilization LIMIt 10")
df1_sql.show()


df1_sql = spark.sql("SELECT * FROM utilization WHERE server_id = 120 LIMIT 10")
df1_sql.show()

df1_sql = spark.sql("SELECT * FROM utilization WHERE session_count > 70")
df1_sql.show()

df1_sql.count()

df1_sql = spark.sql("SELECT server_id, session_count FROM utilization WHERE session_count > 70 AND server_id =120")
df1_sql.show()

df1_sql.count()

df1_sql = spark.sql("SELECT server_id, session_count FROM utilization \
                    WHERE session_count > 70 AND server_id =120 \
                    ORDER BY session_count desc")
df1_sql.show()

df1_sql = spark.sql("SELECT count(*) FROM utilization")
df1_sql.show()

df1_sql = spark.sql("SELECT count(*) FROM utilization \
                     WHERE session_count > 70")
df1_sql.show()

df1_sql = spark.sql("SELECT server_id, count(*) FROM utilization \
                     WHERE session_count > 70 \
                      GROUP BY server_id \
                      ORDER BY count(*) DESC")
df1_sql.show()

df1_sql = spark.sql("SELECT server_id, min(session_count), round(avg(session_count),2), max(session_count) FROM utilization \
                     WHERE session_count > 70 \
                      GROUP BY server_id \
                      ORDER BY count(*) DESC")
df1_sql.show()

csv_df_path = data_path + "/server_name.csv"
# df1_server = spark.read.csv(csv_df_path, header=True)

df1_sql = spark.sql("SELECT DISTINCT server_id from utilization ORDER BY server_id")
df1_sql.show()

spark.sql("SELECT min(server_id), max(server_id) FROM utilization").show()

df_join = spark.sql("SELECT u.server_id, sn.server_name, u.session_count \
                     FROM utilization u INNER JOIN serevr_name sn \
                    ON sn.server_id = u.server_id")



