## Read in semi-structure text data

# Read in raw text
rawData = open('SMSSpamCollection.tsv').read()
# print the raw data
rawData[0:500]

parseData = rawData.replace('\t', '\n').split('\n')

parseData[0:5]

labellist = parseData[0::2]
textList = parseData[1::2]


print(labellist[0:5])
print(textList[0:5])

import pandas as pd

fullCorpus = pd.DataFrame({
    'label' : labellist,
    'body_list' : textList
    
})
fullCorpus.head()

print(len(labellist))
print(len(textList))

print(labellist[-5:]) # means starting from -5 to end 

import pandas as pd
pd.set_option('display.max_colwidth', 100)

fullCorpus = pd.DataFrame({
    'label' : labellist[:-1],
    'body_list' : textList
    
})
fullCorpus.head()

# it can be done with panda read csv file
dataset = pd.read_csv('SMSSpamCollection.tsv', sep="\t", header=None)
dataset.columns = ['label', 'body_text']
dataset.head()

# what is the shape of database
print("Input data has {} rows and {} columns".format(len(fullCorpus), len(fullCorpus.columns)))

#how many spam and ham
print("out of {} rwos, {} are spam, {} are ham".format(len(fullCorpus),len(fullCorpus[fullCorpus['label']=='spam']),
                                                      len(fullCorpus[fullCorpus['label']=='ham'])))

#how much missing data 
print("Number of null in label: {}".format(fullCorpus['label'].isnull().sum()))
print("Number of null in text: {}".format(fullCorpus['body_list'].isnull().sum()))

# cleaned data look like
data_cleaned = pd.read_csv("SMSSpamCollection_cleaned.tsv", sep='\t')
data_cleaned.head()

import string
string.punctuation

def remove_punct(text):
    text_nonpunct = "".join([char for char in text if char not in string.punctuation])
    return text_nonpunct

dataset['body_text_clean'] = dataset['body_text'].apply(lambda x: remove_punct(x))
dataset.head()

import re
def tokenize(text):
    tokens =re.split('\W+', 'text') # \W means split wherever you see non word char like space , . etc
    return tokens

dataset['body_text_tokenized'] = dataset['body_text_clean'].apply(lambda x: remove_punct(x.lower()))
dataset.head()

import ssl
import nltk
from nltk.corpus import stopwords
stopword = nltk.corpus.stopwords.words('english')

def remove_stopwords(tokenized_list):
    text = [word for word in tokenized_list if word not in stopwords]
    return text
dataset['body_text_nostop'] = dataset['body_text_tokenized'].apply(lambda x: remove_stopwords(x))
data.head()

ps = nltk.PorterStemmer()

dir(ps)

print(ps.stem('grows'))
print(ps.stem('growing'))
print(ps.stem('grow'))

def clean_text(text):
    text = "".join([word for word in text if word not in string.punctuation])
    tokens = re.split('\W+', text)
    text = [ word for word in tokens if word not in stopwords]
    return text
dataset['body_text_nonstop'] = dataset['body_text'].apply(lambda x: clean_text(x.lower()))

def stemming(tokenized_text):
    text = [ps.stem(word) for word in tokenized_text]
    return text

dateset['body_text_stemmed'] = dataset['body_text_nonstop'].apply(lambda x: stemming(x))

def lemmatizing(tokenized_text):
    text = [wn.lemmatize(word) for word in tokenized_text]
    return text

dataset['body_text_lemmatized'] = dataset['body_text_nonstop'].apply(lambda x: lemmatizing(x))
dataset.head(10)

